# چارچوب ترکیبی نوروسیمبولیک برای توضیح‌پذیری تحلیل احساسات (BERT/RoBERTa + SenticNet)

این پروژه یک چارچوب ترکیبی نوروسیمبولیک را برای طبقه‌بندی تحلیل احساسات (Sentiment Analysis) روی دیتاست **SST-2** پیاده‌سازی می‌کند. این چارچوب از مدل‌های زبانی **BERT** و **RoBERTa** به همراه دانش نمادین از **SenticNet** بهره می‌برد. هدف اصلی، ترکیب دانش عصبی و نمادین و بررسی توضیح‌پذیری مدل با استفاده از معیار Infidelity است.

## رویکرد

1.  **مدل‌های پایه:** از مدل‌های `bert-base-uncased` و `roberta-base` به عنوان پایه عصبی استفاده شده است (نوت‌بوک شامل کد برای هر دو است).
2.  **دانش نمادین:** از کتابخانه `senticnet` برای استخراج قطبیت (Polarity) میانگین کلمات کلیدی در متن ورودی استفاده می‌شود.
3.  **ترکیب Neuro-Symbolic:** بردار نمایش توکن `[CLS]` (برای BERT) یا `<s>` (برای RoBERTa) از خروجی مدل پایه با مقدار قطبیت استخراج شده از SenticNet ترکیب شده و به یک لایه طبقه‌بند خطی نهایی داده می‌شود.
4.  **انطباق مدل (Fine-tuning):** از روش **TADA انعطاف‌پذیر (Flexible TADA - Proposed)** استفاده شده است. در این روش، تنها پارامترهای لایه Embedding (`embeddings`) و آخرین بلاک Transformer (`encoder.layer.11`) مدل پایه برای آموزش باز گذاشته می‌شوند تا فرآیند Fine-tuning کارآمدتر باشد. حدود 28-37% پارامترها آموزش داده می‌شوند.
5.  **توضیح‌پذیری:** به عنوان یک معیار اولیه توضیح‌پذیری، از **Infidelity** استفاده شده است. این معیار، تغییر (مربع تفاضل) در احتمال پیش‌بینی کلاس صحیح را پس از حذف کلمات کلیدی شناسایی شده توسط SenticNet اندازه‌گیری می‌کند.

## نتایج (با استفاده از Flexible TADA روی زیرمجموعه SST-2)

* **BERT-base:**
    * Accuracy: ~83.50%
    * F1-Score: ~84.79%
    * میانگین Infidelity (روی ۵ نمونه): ~0.072
* **RoBERTa-base:**
    * Accuracy: ~77.00%
    * F1-Score: ~81.45%
    * میانگین Infidelity (روی ۵ نمونه): ~0.008

*(توجه: این نتایج روی زیرمجموعه کوچکی از دیتاست (1000 نمونه آموزشی، 200 نمونه اعتبارسنجی) و با روش TADA انعطاف‌پذیر به دست آمده‌اند)*.

## نحوه اجرا

1.  نصب کتابخانه‌های مورد نیاز: `pip install -r requirements.txt`
2.  اجرای سلول‌های نوت‌بوک `NeuroSymbolic_BERT_RoBERTa_TADA.ipynb`. (نیاز به GPU دارد).